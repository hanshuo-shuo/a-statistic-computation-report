%% \documentclass[addpoints]{exam}
\documentclass[addpoints,answers]{exam}
\input{asset/global_usepackage}
\input{asset/global_newcommand}
\input{asset/global_newtheorem}
\input{asset/global_newenviroment}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx} %插入图片的宏包
\usepackage{float} %设置图片浮动位置的宏包
\usepackage{subfigure} %插入多图时用子图显示的宏包
\usepackage{longtable}
\usepackage{diagbox}
\usepackage{listings}
\usepackage{appendix}
\usepackage{xcolor}
\lstset{
    numbers=left, 
    numberstyle= \tiny, 
    keywordstyle= \color{ blue!70},
    commentstyle= \color{red!50!green!50!blue!50}, 
    frame=shadowbox, % 阴影效果
    rulesepcolor= \color{ red!20!green!20!blue!20} ,
    escapeinside=``, % 英文分号中可写入中文
    xleftmargin=2em,xrightmargin=2em, aboveskip=1em,
    framexleftmargin=2em
   % language=R
} 
\geometry{left=20mm,right=20mm,top=20mm,bottom=20mm}
\DeclareMathOperator{\sech}{sech}

\begin{document}

\begin{center}
  \fbox{\fbox{\parbox{5.5in}{\centering
        2020-2021学年第二学期数学实验\\
        实验报告\\
  }}} 
\end{center}

\vspace{5mm}

\begin{center}
  \makebox[0.3\textwidth]{姓名:\enspace\hrulefill}
  \hspace{5mm}
  \makebox[0.3\textwidth]{学号:\enspace\hrulefill}
  \hspace{5mm}
  \makebox[0.3\textwidth]{截止日期:\enspace2021.6.25}
\end{center}

\pointname{分}
\renewcommand{\solutiontitle}{\noindent\textbf{解:}\enspace}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% \begin{center}
%%   \hqword{题号}
%%   \hpword{分值}
%%   \htword{总分}
%%   \hsword{得分}
%%   \cellwidth{2.2em}
%%   \gradetable[h][questions]
%% \end{center}

\begin{remark}
	本报告分为两部分内容，即
	\begin{itemize}
		\item 利用神经网络求解偏微分方程(三选一）
		\item 利用LSTM预测金融市场(必选)
	\end{itemize}

实验内容应包括：
\begin{itemize}
	\item 数据处理 
	\item 网络架构
	\item 网络参数(包括网络层数、每层的神经元个数等)对训练的影响
	\item 尽可能给出可视化结果， 包括相关结果的图像
	\item 尽可能把报告内容写得丰富一些，这可以自由发挥
	\item 建议大家编译latex文件时，安装Ctex+TexStudio。
	\item 最终的报告中，请把无关部分注释掉。
\end{itemize}
\end{remark}

\section{利用神经网络求解偏微分方程}
  
  \subsection{Schr\"odinger方程} 利用神经网络PINN求解一维非线性Schrodinger方程  
  \begin{subequations}\label{eq:Schrodinger}
    \begin{align}
      i h_t + 0.5 h_{xx} + |h|^2h = 0, & \quad  x\in[-5,5],   t\in (0,\pi/2], \label{eq:schrodinger_ODE}\\
        h(0,x) = 2 \sech (x), & \quad x \in [-5,5] & \label{eq:schrodinger_IC}\\
        h(t,-5) = h(t, 5), & \quad t\in (0,\pi/2] \label{eq:schrodinger_BC1}\\
          h_x(t,-5) = h_x(t, 5), & \quad t\in (0,\pi/2] \label{eq:schrodinger_BC2}
    \end{align}
  \end{subequations}
  其中$i$为虚数符号，$h(x,t) = u(x,t) + i v(x,t)$为复值函数，\eqref{eq:schrodinger_BC1}和\eqref{eq:schrodinger_BC2}为周期边界条件，$\sech$为双曲正弦函数，即
  $$
  \sech x = \frac{2}{e^x + e^{-x}}.
  $$
  \eqref{eq:Schrodinger}可改写为$u$和$v$的偏微分方程组
  \begin{subequations}\label{eq:Schrodinger_uv}
    \begin{align}
      u_t + 0.5 v_{xx} + (u^2+v^2)v = 0,  & \quad  x\in[-5,5],   t\in (0,\pi/2], \label{eq:schrodinger_ODE_u}\\
        v_t - 0.5 u_{xx} - (u^2+v^2)u = 0, & \quad  x\in[-5,5],   t\in (0,\pi/2], \label{eq:schrodinger_ODE_v}\\
          u(0,x) = 2 \sech (x), & \quad x \in [-5,5] & \label{eq:schrodinger_IC_u}\\
          v(0,x) = 0, & \quad x \in [-5,5] & \label{eq:schrodinger_IC_v}\\
          u(t,-5) = u(t, 5), & \quad t\in (0,\pi/2] \label{eq:schrodinger_BC1_u}\\
            v(t,-5) = v(t, 5), & \quad t\in (0,\pi/2] \label{eq:schrodinger_BC1_v}\\
              u_x(t,-5) = u_x(t, 5), & \quad t\in (0,\pi/2] \label{eq:schrodinger_BC2_u}\\
               v_x(t,-5) = v_x(t, 5), & \quad t\in (0,\pi/2] \label{eq:schrodinger_BC2_v}
    \end{align}
  \end{subequations}

  \begin{remark}
    薛定谔方程是一个经典的场方程，用于研究量子力学系统，包括非线性波在光纤和波导中的传播、玻色-爱因斯坦凝聚和等离子体波。在光学中，非线性项来自于给定材料的与强度有关的折射率。类似地，玻色-爱因斯坦凝聚的非线性项是相互作用的多体系统的平均场相互作用的结果。
  \end{remark}

  \begin{figure}[htbp]
    \centering
    \includegraphics[width=5in]{tikz/Net1.pdf}
    \caption{求解薛定谔方程的神经网络架构}
  \end{figure}

 \paragraph{损失函数} 损失函数可表示为
    $$
    \cL(\vTheta) = \cL_{\mathrm{PDE}}(\vTheta) + \lambda_1 \cL_{\mathrm{IC}}(\vTheta) + \lambda_2 \cL_{\mathrm{BC}}(\vTheta)
    $$
    其中
    $$
    \begin{aligned}
      \cL_{\mathrm{PDE}}(\vTheta) = \frac1{N_{\mathrm{PDE}}} \sum_{n=1}^{N_{\mathrm{PDE}}} \left\{  \left[\hat u_t (t_n, x_n) + 0.5 \hat v_{xx}(t_n, x_n) + \left(\hat u(t_n, x_n)^2+\hat v(t_n, x_n)^2\right)\hat v(t_n, x_n) \right]^2 \right. \\
      \quad \left. + \left[\hat v_t(t_n, x_n) - 0.5 \hat u_{xx}(t_n, x_n) - \left(\hat u(t_n, x_n)^2+\hat v(t_n, x_n)^2\right)\hat u(t_n, x_n)\right]^2 \right\},
    \end{aligned}
    $$
    $$
    \begin{aligned}
      \cL_{\mathrm{IC}}(\vTheta) = \frac1{N_{\mathrm{IC}}} \sum_{n=1}^{N_{\mathrm{IC}}} \left\{\left[\hat u(0,x_n) - 2 \sech (x_n)\right]^2 + \hat v(0, x_n)^2 \right\}
    \end{aligned}
    $$
    $$
    \begin{aligned}
      \cL_{\mathrm{BC}}(\vTheta) = \frac1{N_{\mathrm{BC}}} \sum_{n=1}^{N_{\mathrm{BC}}} \left\{
      \left[\hat u(t_n,-5) - \hat u(t_n, 5)\right]^2 + 
      \left[\hat v(t_n,-5) - \hat v(t_n, 5)\right]^2 + \right. \\
      \quad \left. \left[\hat u_x(t_n,-5) - \hat u_x(t_n, 5)\right]^2 +
      \left[\hat v_x(t_n,-5) - \hat v_x(t_n, 5)\right]^2 
      \right\}
    \end{aligned}
    $$
    为方便, 这里用$\hat u$和$\hat v$表示网络的输出, 即
    $$
    \hat u(t, x) := u(t, x; \vTheta), ~~
    \hat u(t, x) := v(t, x; \vTheta).
    $$


  

  

 
\section{利用LSTM预测金融市场}
\subsection{数据处理}
由于需要预测股票涨跌，所以我们首先对每一天的股票涨跌情况打上标签。规则为：当股票价格的变化量大于所有股票价格变化量的中位数时，就认为股票上涨，在数据集中记为1，而当股票价格变化量小于所有股票价格变化量中位数时认为股票下跌，在数据集中标记为0。我们选取240天的股票价格数据和第241天的股票涨跌情况组成一个样本标签对。

由于需要使用LSTM网络，我们对样本标签集进行了维度上的变化。样本集最终维度为（样本数，240，1），标签集的维度为（样本数，1）。

在数据处理过程中，我们采用了以下函数：
\begin{itemize}
    \item centralize(data)
    对数据进行中心化
    \item judge(dataset,k)
    判断第k天各支股票的涨跌情况。
    \item create\_dataset(dataset,look\_back=240)
    生成样本标签对
\end{itemize}

具体数据处理相关代码如下：
\begin{lstlisting}
def centralize(data):
    min_value = np.min(data,axis=0)
    max_value = np.max(data,axis=0)
    data = (data - min_value) / (max_value-min_value)
    return data

def judge(dataset,k):
    '''
    to see at day k, if each stock rise or fall
    '''
    pr_today = dataset[k]
    pr_yesterday = dataset[k-1]
    pr_change = pr_today - pr_yesterday
    med = np.median(pr_change)
    re = np.zeros_like(pr_change)
    re[pr_change>0] = 1
    return(re)

def create_dataset(dataset,look_back=240):
    dataX,dataY=[],[]
    for i in range(len(dataset)-look_back):
        pr_change = judge(dataset,i+look_back)
        a = dataset[i:(i+look_back)]
        dataX.append(a)
        dataY.append(pr_change)
    return np.array(dataX),np.array(dataY)

look_back = 240
index_used = index[look_back:]
index_used = np.array(index_used)
X, Y = create_dataset(dataset,look_back)
print(X.shape, Y.shape)
a,b,c = X.shape

train_size = int(len(X) * 0.9)
valid_size = len(X) - train_size
index_size = int(len(index_used)*0.9)
print(train_size, valid_size)

X_train = X[:train_size]
Y_train = Y[:train_size]
index_train = index_used[:index_size]


X_valid = X[train_size:]
Y_valid = Y[train_size:]
index_valid = index_used[index_size:]


# X_train = X_train.reshape(-1,198,240)
# X_valid = X_valid.reshape(-1,198,240)
# Y_train = Y_train.reshape(-1,198,1)

X_train = X_train.reshape(train_size*c,b,1)
Y_train = Y_train.reshape(train_size*c,1)
X_valid = X_valid.reshape(valid_size*c,b,1)
Y_valid = Y_valid.reshape(valid_size*c,1)

# X_train = X_train.transpose(1, 0, 2)
# X_valid = X_valid.transpose(1, 0, 2)

X_train = torch.from_numpy(X_train)
Y_train = torch.from_numpy(Y_train)
X_valid = torch.from_numpy(X_valid)

print(X_train.shape,Y_train.shape)
\end{lstlisting}


\subsection{算法解释及网络结构}
LSTM全程为Long Short-Term Memory,顾名思义，它具有记忆长短期信息的能力的神经网络。LSTM首先在1997年由Hochreiter 和Schmidhuber提出，由于深度学习在2012年的兴起，LSTM又经过了若干代大牛(Felix Gers, Fred Cummins, Santiago Fernandez, Justin Bayer,Daan Wierstra, Julian Togelius, Faustino Gomez, Matteo Gagliolo, and Alex Gloves)的发展，由此便形成了比较系统且完整的LSTM框架，并且在很多领域得到了广泛的应用，尤其是对于时间序列数据常常能达到很好的效果。
而对于股票价格数据，则很适合采用LSTM网络来分析和处理。
\begin{figure}[H] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
\centering %图片居中
\includegraphics[width=0.7\textwidth]{templates/lstm.png} %插入图片，[]中设置图片大小，{}中是图片文件名
\caption{LSTM网络图解} %最终文档中希望显示的图片标题
\label{Fig.main2} %用于文内引用的标签
\end{figure}

LSTM内部主要有三个阶段：
\begin{itemize}
    \item 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会“忘记不重要的，记住重要的”。即利用$z^f$来控制上个阶段传入的$c^{t-1}$被遗忘的程度。
    \item 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入$x^t$ 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的$z$ 表示。而选择的门控信号则是由$z^i$（i代表information）来进行控制
    \item 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过$z^0$ 来进行控制的。并且还对上一阶段得到的$c^0$进行了放缩（通过一个tanh激活函数进行变化）。
\end{itemize}
    
由于本次报告采用pytorch框架下的LSTM网络，所以在此解释所需要关注的网络参数。
\begin{itemize}
    \item input\_size 输入数据的大小
    \item hidden\_size 隐藏层大小
    \item num\_laryers 循环神经网络层数
\end{itemize}

而LSTM有单向LSTM和双向LSTM之分，单向LSTM主要采用过去的信息，而双向LSTM既采用过去的信息，又采用现在的信息。对于本次实验，单向LSTM主要是将所有股票无区别对待，不考虑股票之间的相互影响，直接利用样本标签对进行训练和验证。而双向LSTM则考虑股票之间的影响，将同一时间段的所有股票数据看作一个样本标签对。反映到LSTM网络搭建中，则是单向LSTM网络中我们把input\_size和num\_layers设为1，而隐藏层数目则在后面会进行多样性分析。在LSTM网络之后，我们还连接了一个线性全连接网络以完成判别工作。为了线性网络能与LSTM顺利连接，其in\_features与LSTM网络的hidden\_size相同，out\_features为1。类似地，我们在后面也对不同的损失函数和优化器作了对比。对于双向LSTM网络，我们把input\_size设为240，num\_layers设为2。类似地，我们依旧连接了一个全连接网络，但是由于这里是双向LSTM，所以我们把in\_features设为2倍的hidden\_size，而out\_features依旧为1。

至此，网络的基本结构已经搭建完成。但为了更好的得到以及呈现结果，我们小组也添加了一些其他代码及函数。
\begin{itemize}
    \item acc(out,y\_real) 对判别结果的正确率进行计算，输入参数为真实判别值和预测判别值。
    \item set\_seed(seed)
    设置种子，使得每次结果一样，便于检测代码错误。
    \item 采用DataLoader来做batch
\end{itemize}
具体网络搭建代码如下：
\begin{lstlisting}
%%time
class LSTMRegression(nn.Module):
    def __init__(self, input_size, hidden_size, output_size=1, num_layers=1):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,batch_first=True)
        self.linear = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        _, (hn, cn) = self.lstm(x)
        hn = hn.squeeze()
        out = self.linear(hn)
        return out

model = LSTMRegression(input_size=1, hidden_size=5, output_size=1)

criterion = torch.nn.BCEWithLogitsLoss()     #交叉熵BCEWithLogitsLoss()和MultiLabelSoftMarginLoss()
#criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)
#optimizer = optim.SGD(model.parameters(), lr=1e-1)

epochs = 100
batch_size = 30
batch = X_train.shape[0] // batch_size



torch_dataset = Data.TensorDataset(torch.tensor(X_train), torch.tensor(Y_train))
# 把 dataset 放入 DataLoader
loader = Data.DataLoader(
    dataset=torch_dataset,  # torch TensorDataset format
    batch_size=batch_size,  # mini batch size
    shuffle=True,  #
    num_workers=10,  # 多线程来读数据
)

loss_epoch = np.zeros(epochs)
acc_epoch = np.zeros(epochs)
loss_valid = np.zeros(epochs)
acc_valid = np.zeros(epochs)
for epoch in range(epochs):
    loss_ep = np.array([])
    acc_ep = np.array([])
    loss_epv = np.array([])
    acc_epv = np.array([])
    for step,(var_x,var_y) in enumerate(loader):
        out = model(var_x)
        out_f = out.detach().clone().numpy()
        var_yf = var_y.detach().clone().numpy()
        loss = criterion(out, var_y)
        loss_f = loss.detach().clone().numpy()
        acc_ep = np.append(acc_ep,acc(out_f,var_yf))
        loss_ep = np.append(loss_ep,loss_f)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()  
    
    if (epoch + 1) % 5 == 0:
        print(f'Epoch: {epoch:5d}, Loss: {np.mean(loss_ep):.4e}, Acc:{np.mean(acc_ep):.4e}')


    loss_epoch[epoch] = np.mean(loss_ep)
    acc_epoch[epoch] = np.mean(acc_ep)
    
    Y_pre = model(X_valid)  #计算验证集表现
    Y_pre1 = Y_pre.clone().detach().numpy()
    Y_valid1 = torch.from_numpy(Y_valid)
    loss_valid[epoch] = criterion(Y_pre,Y_valid1)
    acc_valid[epoch] = acc(Y_pre1,Y_valid)
\end{lstlisting}

为了更好地呈现结果，我们采取了一系列可视化工作，主要画出了以下图像：
\begin{itemize}
    \item loss随着epoch变化的图像
    \item 准确率随着epoch变化的图像
    \item 模型在验证集上的预测判别值和真实涨跌情况对比
\end{itemize}
具体代码如下：
\begin{lstlisting}
    # visulize
    kind = 2
    series = np.arange(kind*len(index_valid),(kind+1)*len(index_valid))
    Y_pred_re = Y_pred
    Y_pred_re[Y_pred_re>0] = 1
    Y_pred_re[Y_pred_re<=0] = 0

    
    filename1 = 'point' + str(index) + '.png'
    filename2 = 'pic' + str(index) + '.png'
    
    fig = plt.figure()
    ax = plt.subplot()
    type1 = ax.scatter(index_valid, Y_valid[series], alpha=0.6,color='b',label='groundtruth') 
    type2 = ax.scatter(index_valid, Y_pred_re[series], alpha=0.3,color='r',label='prediction') 
    plt.xlabel("date time")
    plt.ylabel("0 for fall, 1 for rise")
    ax.legend((type1, type2), (u'groundtruth', u'prediction'), loc='best')
    plt.savefig(filename1)
    plt.show()

    plt.plot(loss_epoch, 'r-', label='loss')
    plt.plot(acc_epoch, 'b-', label='accurate rate')
    plt.legend(loc='best')
    plt.savefig(filename2)
    plt.show()

\end{lstlisting}
最后，采用股票价格数据训练模型。由于数据量较大，所以我们只训练了100轮，得到的loss和正确率图像如下图所示：
% \begin{figure}[H]
% \centering
% \begin{minipage}[t]{0.48\textwidth}
% \centering
% \includegraphics[width=6cm]{templates/general/acc.png}
% \caption{单向LSTM网络训练集和验证集上的正确率变化}
% \end{minipage}
% \begin{minipage}[t]{0.48\textwidth}
% \centering
% \includegraphics[width=6cm]{templates/general/loss.png}
% \caption{单向LSTM网络训练集和验证集上的loss值变化}
% \end{minipage}
% \end{figure}

\begin{figure}[H] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
\centering %图片居中
\includegraphics[width=0.7\textwidth]{templates/general/acc.png} %插入图片，[]中设置图片大小，{}中是图片文件名
\caption{单向LSTM网络训练集和验证集上的正确率变化} %最终文档中希望显示的图片标题
\label{Fig.main3} %用于文内引用的标签
\end{figure}

\begin{figure}[H] %H为当前位置，!htb为忽略美学标准，htbp为浮动图形
\centering %图片居中
\includegraphics[width=0.7\textwidth]{templates/general/loss.png} %插入图片，[]中设置图片大小，{}中是图片文件名
\caption{单向LSTM网络训练集和验证集上的loss值变化} %最终文档中希望显示的图片标题
\label{Fig.main4} %用于文内引用的标签
\end{figure}
由以上两幅图可以发现，对于单向LSTM网络，在训练了20轮到80轮时得到的模型在验证集上表现较好且较稳定。到80轮之后验证集上的判断正确率产生了较大下降，猜测可能是产生了过拟合或者是网络超参数设置不当。所以，我们在后面主要对网络参数进行了分析实验，轮数一般设置在20到100之间。

对于双向LSTM网络，我们同样得到训练集和验证集的loss和准确率的变化图像，如下：

\begin{figure}[H]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/double/dacc.png}
\caption{双向LSTM网络训练集和验证集上的正确率变化}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/double/dloss.png}
\caption{双向LSTM网络训练集和验证集上的loss变化}
\end{minipage}
\end{figure}

由两幅图的纵坐标可以看到，无论是验证集还是训练集，loss和准确率变化并不大，由此可见双向LSTM网络不适合处理股票数据或者网络参数设置有问题。

%\lstinputlisting{code/单向lstm.ipynb} %花括号内为文件的路径
%\lstinputlisting{code/双向lstm.ipynb}




\subsection{不同的hidden size/神经元数目对结果的影响}\footnote{本节代码附在报告最后}
由于input size和output size固定，所以我们主要通过改变hidden size来改变神经元数目，进而进行分析。

我们分别对隐藏层为3，4，5的情况进行了实验，得到的损失函数值和准确率随时间变化图像如下所示：
\begin{figure}[H]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/hidden/h3loss.png}
\caption{hidden size=3}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/hidden/h4loss.png}
\caption{hidden size=4}
\end{minipage}\\
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/hidden/h5loss.png}
\caption{hidden size=5}
\end{minipage}
\end{figure}

训练得到的模型在验证集上的表现如下所示：
\begin{figure}[H]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/hidden/h3acc.png}
\caption{hidden size=3}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/hidden/h4acc.png}
\caption{hidden size=4}
\end{minipage}\\
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/hidden/h5acc.png}
\caption{hidden size=5}
\end{minipage}
\end{figure}
根据以上结果不难看出，这三种hidden size的结果都比较好，虽然中间有所波动，但最终得到的正确率都比较高。并且，隐藏层为5时得到的模型的正确率变化最稳定，效果最好。

而对于双向LSTM网络，我们的训练过程中的loss和正确率的变化如下：
\begin{figure}[H]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/hidden/双向h2.png}
\caption{hidden size=2}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/hidden/双向h3.png}
\caption{hidden size=3}
\end{minipage}\\
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/hidden/双向h4.png}
\caption{hidden size=4}
\end{minipage}
\end{figure}
我们可以看出，对于双向LSTM网络，在训练集上的表现并不好，loss和正确率基本不变。这说明双向LSTM网络可能不太适合对股票数据进行判断，因此，在报告的后面我们也是主要集中于对单向LSTM网络的分析。
\subsection{不同的batch size对结果的影响}\footnote{本节代码附在报告最后}
利用控制变量的方法，保持其他变量不变，改变batch\_size，在训练20轮的情况下，分析单向lstm网络的训练结果。每训练2个epoch，打印一次相应的损失函数loss和准确率acc。batch\_size影响模型的训练过程，进而影响模型的性能。一方面影响模型的收敛时间，另一方面影响训练好的模型的泛化能力即在验证集上的效果。下面通过不同的batch\_size来探讨这个问题并选出较好的参数。
训练过程中，随着训练轮数增加，通过观察在训练集上acc和loss的变化，可以知道模型的收敛程度。在下图中，分别画出了batch\_size为10，30，60的情况下，训练20轮的过程中，acc和loss的变化。

  \begin{figure}[H]
    \centering
    \includegraphics[width=3in]{templates/batch_size/batchsize10.png}
    \caption{batch size=10}
  \end{figure}

  \begin{figure}[H]
    \centering
    \includegraphics[width=3in]{templates/batch_size/batchsize30.png}
    \caption{batch size=30}
  \end{figure}

  \begin{figure}[H]
    \centering
    \includegraphics[width=3in]{templates/batch_size/batchsize60.png}
    \caption{batch size=60,epoch=20}
  \end{figure}

\subsection{不同的学习率对结果的影响}\footnote{本节代码附在报告最后}
 利用控制变量的方法，保持其他变量不变，改变学习率lr(learning rate)，在训练30轮的情况下，分析单向lstm网络的训练结果。每训练2个epoch，打印一次相应的损失函数loss和准确率acc。学习率的影响体现为，学习率越大，输出误差对参数的影响就越大，参数更新的就越快，但同时受到异常数据的影响也就越大，很容易发散。并且在一方面影响模型的收敛时间，另一方面影响训练好的模型的泛化能力即在验证集上的效果。下面通过不同的lr来探讨这个问题并选出较好的参数。
训练过程中，随着训练轮数增加，通过观察在训练集上acc和loss的变化，可以知道模型的收敛程度。在下图中，分别画出了学习率为1e-2,1e-3.1e-4的情况下，训练30轮的过程中，acc和loss的变化。
\\
以下是随训练轮数增加，预测的股票涨跌预测情况：
\begin{figure}[H]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/lr/loss and accuracy 1e-2.png}
\caption{lr=0.01}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/lr/loss and accuracy 1e-3.png}
\caption{lr=0.001}
\end{minipage}\\
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/lr/loss and accuracy 1e-4.png}
\caption{lr=0.0001}
\end{minipage}
\end{figure}

以下列出了训练过程中的acc。

\begin{figure}[H]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/lr/fall and rise 1e-2.png}
\caption{lr=0.01，验证集结果}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/lr/fall and rise 1e-3.png}
\caption{lr=0.001，验证集结果}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/lr/fall and rise 1e-4.png}
\caption{lr=0.0001，验证集结果}
\end{minipage}
\end{figure}

运行时间上，学习率越小，运行30轮的时间越久，但准确率也随之提高。lr=1e-2运行时间最短,1r=1e-4准确率最高。

收敛速度上，30轮过后，三个lr对应的loss都降到了0.4以下，但具体来看，lr=1e-2时，loss呈现周期性的下降，说明学习率过大，不具有稳定性，lr=1e-3时，周期性明显改善，但最终仍有上升的趋势，lr=1e-4时，loss的下降非常光滑。
三种方法的准确率最终都达到0.8左右，说明该方法的在训练集上表现较好。

在验证集中验证模型，得到不同lr对应的在验证集中的acc。根据该数据，可以考察模型的泛化能力。
对应1e-2，1e-3，1e-4，训练完在验证集上的准确率分别为0.625，0.901，0.918。
比较三点在验证集上的准确率，lr=1e-4的模型达到了三个中的最高值0.918


\subsection{不同的优化器对结果的影响}\footnote{本节代码附在报告最后}
在这一部分，我们考虑了Adam和SGD优化器，并对结果进行了检验。对于Adam优化器，我们发现最终在验证集下的判断准确率为0.919，对于SGD优化器，我们得到的验证集判断准确率为0.747。因此对于股票数据，可能Adam优化器更合适。我们的训练过程中loss函数和准确率随着训练轮数的变化如下图所示：
\begin{figure}[H]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/optimizer/pic0.png}
\caption{SGD}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/optimizer/pic1.png}
\caption{Adam}
\end{minipage}
\end{figure}
我们可以看到，Adam优化器对应的损失函数值在中间经历了一段上升过程，分析可能的原因是陷入了局部最小，可以将学习率适当调小。最后，我们给出由训练出来的模型在验证集上的判断表现：
\begin{figure}[H]
\centering
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/optimizer/point0.png}
\caption{SGD}
\end{minipage}
\begin{minipage}[t]{0.48\textwidth}
\centering
\includegraphics[width=8cm]{templates/optimizer/point1.png}
\caption{Adam}
\end{minipage}
\end{figure}

\newpage
\section*{附录}
\appendix
\section{神经网络求解偏微分方程代码}
\section{LSTM预测股票涨跌代码}
\subsection{主体代码}
单向LSTM网络
\lstinputlisting{templates/all_code/单向主体.py}
双向LSTM网络
\lstinputlisting{templates/all_code/双向主体.py}
\subsection{不同的hidden size/神经元数目对结果的影响的代码}
单向LSTM网络
\lstinputlisting{templates/all_code/单向hiddensize.py}
双向LSTM网络
\lstinputlisting{templates/all_code/双向hiddensize.py}
\subsection{不同的batch size对结果的影响的代码}
\lstinputlisting{templates/all_code/单向batchsize.py}
\subsection{不同的学习率对结果的影响的代码}
\lstinputlisting{templates/all_code/单向学习率.py}
\subsection{不同的优化器对结果的影响的代码}
\lstinputlisting{templates/all_code/单向优化器.py}


\end{document}
